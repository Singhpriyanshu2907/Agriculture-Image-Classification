{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-1g8MtjG0Yx"
   },
   "source": [
    "**Tensorflow Version == 2.17\n",
    "** Python Version == 3.10.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sq2oq9wb6NtN"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img\n",
    "from keras.regularizers import l2\n",
    "from PIL import UnidentifiedImageError\n",
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications import EfficientNetB0, Xception, MobileNetV2, NASNetMobile\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, Dropout\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import random\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26349,
     "status": "ok",
     "timestamp": 1728080150030,
     "user": {
      "displayName": "priyanshu singh",
      "userId": "03469869412728691574"
     },
     "user_tz": -330
    },
    "id": "upByOhMO7Zty",
    "outputId": "66cdd3f5-5892-4ada-f989-3597aec174cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufLqgAf47Zr8"
   },
   "outputs": [],
   "source": [
    "data_path = \"/content/drive/MyDrive/Agriculture Classifier/Data\"\n",
    "dir_path = \"/content/drive/MyDrive/Agriculture Classifier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PYUHUAOC7ZpJ"
   },
   "outputs": [],
   "source": [
    "train_dir = os.path.join(dir_path, 'train')\n",
    "val_dir = os.path.join(dir_path, 'val')\n",
    "\n",
    "# Create directories if they do not exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(val_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5pi9nKIJ7ZkF"
   },
   "outputs": [],
   "source": [
    "# Function to check images in all subdirectories\n",
    "def check_images(directory):\n",
    "    invalid_images = []  # List to store paths of invalid images\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            # Only process image files (add more extensions if needed)\n",
    "            if file.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):\n",
    "                img_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    img = Image.open(img_path)\n",
    "                    img.verify()  # Check if the image can be opened\n",
    "                except (IOError, SyntaxError) as e:\n",
    "                    invalid_images.append(img_path)  # Store invalid image paths\n",
    "\n",
    "    # Print summary\n",
    "    if invalid_images:\n",
    "        print(\"The following images are corrupted or in an invalid format:\")\n",
    "        for img_path in invalid_images:\n",
    "            print(f\"- {img_path}\")\n",
    "    else:\n",
    "        print(\"All images are in the correct format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 469742,
     "status": "ok",
     "timestamp": 1728080648539,
     "user": {
      "displayName": "priyanshu singh",
      "userId": "03469869412728691574"
     },
     "user_tz": -330
    },
    "id": "znxB1XaV7ZhT",
    "outputId": "c98a916c-bb9c-4883-b623-c320927781ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images are in the correct format.\n"
     ]
    }
   ],
   "source": [
    "# Check images in all subdirectories\n",
    "check_images(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgT2RM_M7ZfN"
   },
   "outputs": [],
   "source": [
    "# Paths\n",
    "data_path = \"/content/drive/MyDrive/Agriculture Classifier/Data\"\n",
    "train_path = \"/content/drive/MyDrive/Agriculture Classifier/train\"\n",
    "val_path = \"/content/drive/MyDrive/Agriculture Classifier/val\"\n",
    "\n",
    "# Define the train/validation split ratio (e.g., 80% train, 20% validation)\n",
    "split_ratio = 0.2\n",
    "\n",
    "# Ensure train and val directories exist\n",
    "os.makedirs(train_path, exist_ok=True)\n",
    "os.makedirs(val_path, exist_ok=True)\n",
    "\n",
    "# Function to split data into train and validation sets\n",
    "def split_data():\n",
    "    for category in os.listdir(data_path):\n",
    "        category_path = os.path.join(data_path, category)\n",
    "\n",
    "        # Debugging: Print category path and files\n",
    "        print(f\"Processing category: {category}, Path: {category_path}\")\n",
    "\n",
    "        if os.path.isdir(category_path):\n",
    "            # Get list of all images in the category folder (filter valid image files)\n",
    "            images = [f for f in os.listdir(category_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "            # Debugging: Print found images\n",
    "            print(f\"Found {len(images)} images in {category}\")\n",
    "\n",
    "            # Skip the category if there are no valid images\n",
    "            if len(images) == 0:\n",
    "                print(f\"Skipping category {category}: no images found.\")\n",
    "                continue\n",
    "\n",
    "            # Split the images into training and validation sets\n",
    "            train_images, val_images = train_test_split(images, test_size=split_ratio, random_state=42)\n",
    "\n",
    "            # Create corresponding train and val directories for the category\n",
    "            category_train_path = os.path.join(train_path, category)\n",
    "            category_val_path = os.path.join(val_path, category)\n",
    "            os.makedirs(category_train_path, exist_ok=True)\n",
    "            os.makedirs(category_val_path, exist_ok=True)\n",
    "\n",
    "            # Move train images\n",
    "            for image in train_images:\n",
    "                src_image_path = os.path.join(category_path, image)\n",
    "                dest_image_path = os.path.join(category_train_path, image)\n",
    "                shutil.move(src_image_path, dest_image_path)  # Move the image to the train folder\n",
    "\n",
    "            # Move validation images\n",
    "            for image in val_images:\n",
    "                src_image_path = os.path.join(category_path, image)\n",
    "                dest_image_path = os.path.join(category_val_path, image)\n",
    "                shutil.move(src_image_path, dest_image_path)  # Move the image to the validation folder\n",
    "\n",
    "            print(f\"Processed category: {category}, Train: {len(train_images)}, Validation: {len(val_images)}\")\n",
    "\n",
    "# Run the data split\n",
    "split_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfnDTWt97ZcK"
   },
   "outputs": [],
   "source": [
    "# Function to display 2 images from each category\n",
    "def display_images(folder_path, title):\n",
    "    # Iterate through each category folder\n",
    "    for category in os.listdir(folder_path):\n",
    "        category_path = os.path.join(folder_path, category)\n",
    "\n",
    "        # Check if it's a directory\n",
    "        if os.path.isdir(category_path):\n",
    "            # Get all image files in the category folder\n",
    "            images = [img for img in os.listdir(category_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "            # Select 2 images (or fewer if less than 2 images exist)\n",
    "            selected_images = images[:2]\n",
    "\n",
    "            # Plot the images\n",
    "            fig, axs = plt.subplots(1, len(selected_images), figsize=(10, 5))\n",
    "            fig.suptitle(f'{title} - Category: {category}', fontsize=16)\n",
    "\n",
    "            for i, img_file in enumerate(selected_images):\n",
    "                img_path = os.path.join(category_path, img_file)\n",
    "                img = mpimg.imread(img_path)\n",
    "                axs[i].imshow(img)\n",
    "                axs[i].axis('off')\n",
    "                axs[i].set_title(f\"{img_file}\")\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "# Display 2 images from each category in the train and val folders\n",
    "print(\"Displaying images from TRAIN folder:\")\n",
    "display_images(train_path, \"Train\")\n",
    "\n",
    "print(\"Displaying images from VAL folder:\")\n",
    "display_images(val_path, \"Validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1tDLI1zPbK_"
   },
   "outputs": [],
   "source": [
    "# Load the images from the training directory\n",
    "train_dir = \"/content/drive/MyDrive/Agriculture Classifier/train\"\n",
    "val_dir = \"/content/drive/MyDrive/Agriculture Classifier/val\"\n",
    "\n",
    "# Image parameters\n",
    "img_height, img_width = 224, 224  # VGG16 input size\n",
    "batch_size = 32\n",
    "\n",
    "# Data Augmentation and Preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,            # Normalize pixel values\n",
    "    rotation_range=40,         # Randomly rotate images\n",
    "    width_shift_range=0.2,     # Randomly shift the width\n",
    "    height_shift_range=0.2,    # Randomly shift the height\n",
    "    shear_range=0.2,           # Shear transformation\n",
    "    zoom_range=0.2,            # Zoom in/out\n",
    "    horizontal_flip=True,      # Randomly flip images\n",
    "    fill_mode='nearest'        # Fill in pixels after transformations\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)  # Only rescale validation data\n",
    "\n",
    "# Load train and validation datasets\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UffM3fxHsOnb"
   },
   "outputs": [],
   "source": [
    "# Image parameters\n",
    "img_height, img_width = 224, 224  # VGG16 input size\n",
    "batch_size = 32\n",
    "\n",
    "# Functional API - Define the input layer\n",
    "input_tensor = Input(shape=(img_height, img_width, 3))  # Explicit input shape\n",
    "\n",
    "# Load VGG16 pre-trained model, excluding the top layer (for classification)\n",
    "vgg16_base = VGG16(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "# Freeze the VGG16 layers to avoid retraining them\n",
    "for layer in vgg16_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of VGG16 base\n",
    "x = vgg16_base.output  # Output from the base model\n",
    "x = Flatten()(x)  # Flatten the output\n",
    "x = Dense(256, activation='relu')(x)  # Fully connected layer\n",
    "x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "output_tensor = Dense(30, activation='softmax')(x)  # Output layer for 30 categories\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=1e-4)  # Adam optimizer with a small learning rate\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9dfyTlbuGYp"
   },
   "outputs": [],
   "source": [
    "# Callbacks for learning rate reduction and early stopping\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=3,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=7,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 487907,
     "status": "ok",
     "timestamp": 1728082641156,
     "user": {
      "displayName": "priyanshu singh",
      "userId": "03469869412728691574"
     },
     "user_tz": -330
    },
    "id": "gxdnRQvl7ZUx",
    "outputId": "10322b80-e08a-4e5c-c136-8ba43ca4f41e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - accuracy: 0.0412 - loss: 3.6491 - val_accuracy: 0.0904 - val_loss: 3.2509 - learning_rate: 1.0000e-04\n",
      "Epoch 2/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 560ms/step - accuracy: 0.0948 - loss: 3.2844 - val_accuracy: 0.1695 - val_loss: 3.0871 - learning_rate: 1.0000e-04\n",
      "Epoch 3/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 542ms/step - accuracy: 0.1182 - loss: 3.1488 - val_accuracy: 0.1864 - val_loss: 2.9631 - learning_rate: 1.0000e-04\n",
      "Epoch 4/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 549ms/step - accuracy: 0.1540 - loss: 3.0459 - val_accuracy: 0.2147 - val_loss: 2.8608 - learning_rate: 1.0000e-04\n",
      "Epoch 5/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 469ms/step - accuracy: 0.2038 - loss: 2.8871 - val_accuracy: 0.2542 - val_loss: 2.7325 - learning_rate: 1.0000e-04\n",
      "Epoch 6/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 483ms/step - accuracy: 0.2238 - loss: 2.7768 - val_accuracy: 0.2825 - val_loss: 2.6980 - learning_rate: 1.0000e-04\n",
      "Epoch 7/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 573ms/step - accuracy: 0.2529 - loss: 2.7551 - val_accuracy: 0.2768 - val_loss: 2.5755 - learning_rate: 1.0000e-04\n",
      "Epoch 8/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 554ms/step - accuracy: 0.2535 - loss: 2.6875 - val_accuracy: 0.2938 - val_loss: 2.5149 - learning_rate: 1.0000e-04\n",
      "Epoch 9/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 552ms/step - accuracy: 0.2740 - loss: 2.5978 - val_accuracy: 0.3503 - val_loss: 2.4384 - learning_rate: 1.0000e-04\n",
      "Epoch 10/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 525ms/step - accuracy: 0.2987 - loss: 2.5036 - val_accuracy: 0.3729 - val_loss: 2.3523 - learning_rate: 1.0000e-04\n",
      "Epoch 11/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 484ms/step - accuracy: 0.3175 - loss: 2.4837 - val_accuracy: 0.3333 - val_loss: 2.3560 - learning_rate: 1.0000e-04\n",
      "Epoch 12/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 477ms/step - accuracy: 0.3642 - loss: 2.2648 - val_accuracy: 0.4068 - val_loss: 2.2665 - learning_rate: 1.0000e-04\n",
      "Epoch 13/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 551ms/step - accuracy: 0.3361 - loss: 2.3035 - val_accuracy: 0.4181 - val_loss: 2.2161 - learning_rate: 1.0000e-04\n",
      "Epoch 14/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 493ms/step - accuracy: 0.3680 - loss: 2.2654 - val_accuracy: 0.4350 - val_loss: 2.1400 - learning_rate: 1.0000e-04\n",
      "Epoch 15/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 545ms/step - accuracy: 0.4023 - loss: 2.1974 - val_accuracy: 0.4576 - val_loss: 2.1213 - learning_rate: 1.0000e-04\n",
      "Epoch 16/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 625ms/step - accuracy: 0.4093 - loss: 2.1226 - val_accuracy: 0.4520 - val_loss: 2.1013 - learning_rate: 1.0000e-04\n",
      "Epoch 17/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 542ms/step - accuracy: 0.4363 - loss: 2.0708 - val_accuracy: 0.4689 - val_loss: 2.0531 - learning_rate: 1.0000e-04\n",
      "Epoch 18/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 501ms/step - accuracy: 0.4642 - loss: 1.9623 - val_accuracy: 0.4859 - val_loss: 2.0187 - learning_rate: 1.0000e-04\n",
      "Epoch 19/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 519ms/step - accuracy: 0.4396 - loss: 1.9811 - val_accuracy: 0.4802 - val_loss: 1.9988 - learning_rate: 1.0000e-04\n",
      "Epoch 20/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 521ms/step - accuracy: 0.4510 - loss: 2.0751 - val_accuracy: 0.4859 - val_loss: 1.9740 - learning_rate: 1.0000e-04\n",
      "Epoch 21/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 527ms/step - accuracy: 0.4707 - loss: 1.8911 - val_accuracy: 0.5085 - val_loss: 1.9395 - learning_rate: 1.0000e-04\n",
      "Epoch 22/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 546ms/step - accuracy: 0.4483 - loss: 1.9996 - val_accuracy: 0.5198 - val_loss: 1.9133 - learning_rate: 1.0000e-04\n",
      "Epoch 23/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 558ms/step - accuracy: 0.5040 - loss: 1.8689 - val_accuracy: 0.5311 - val_loss: 1.9124 - learning_rate: 1.0000e-04\n",
      "Epoch 24/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 538ms/step - accuracy: 0.5080 - loss: 1.8334 - val_accuracy: 0.4859 - val_loss: 1.8809 - learning_rate: 1.0000e-04\n",
      "Epoch 25/25\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 604ms/step - accuracy: 0.4759 - loss: 1.8733 - val_accuracy: 0.5424 - val_loss: 1.8737 - learning_rate: 1.0000e-04\n",
      "Restoring model weights from the end of the best epoch: 25.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 217ms/step - accuracy: 0.5248 - loss: 1.9359\n",
      "Validation Loss: 1.873717188835144, Validation Accuracy: 0.5423728823661804\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=25,  # Adjust the number of epochs as needed\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on validation set\n",
    "val_loss, val_acc = model.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEZZSnpI7ZR0"
   },
   "outputs": [],
   "source": [
    "# Load VGG19 pre-trained model, excluding the top layer (for classification)\n",
    "vgg19_base = VGG19(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "# Freeze the VGG19 layers to avoid retraining them\n",
    "for layer in vgg19_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of VGG19 base\n",
    "x = vgg19_base.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_tensor = Dense(30, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model2 = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "# Compile and train\n",
    "model2.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 132825,
     "status": "ok",
     "timestamp": 1728083411767,
     "user": {
      "displayName": "priyanshu singh",
      "userId": "03469869412728691574"
     },
     "user_tz": -330
    },
    "id": "IPPo2BaB7ZOx",
    "outputId": "6f923db2-1600-4610-8d0a-c065634c4735"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 654ms/step - accuracy: 0.0509 - loss: 3.7405 - val_accuracy: 0.0847 - val_loss: 3.2843 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 549ms/step - accuracy: 0.0822 - loss: 3.3655 - val_accuracy: 0.1017 - val_loss: 3.1954 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 536ms/step - accuracy: 0.0962 - loss: 3.2313 - val_accuracy: 0.2034 - val_loss: 3.0645 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 619ms/step - accuracy: 0.1114 - loss: 3.1576 - val_accuracy: 0.1751 - val_loss: 3.0084 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 620ms/step - accuracy: 0.1410 - loss: 3.0674 - val_accuracy: 0.2147 - val_loss: 2.9227 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 703ms/step - accuracy: 0.1535 - loss: 3.0310 - val_accuracy: 0.2712 - val_loss: 2.8293 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 570ms/step - accuracy: 0.1753 - loss: 2.9467 - val_accuracy: 0.2768 - val_loss: 2.7777 - learning_rate: 1.0000e-04\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 230ms/step - accuracy: 0.0615 - loss: 3.2756\n",
      "Validation Loss: 3.2842578887939453, Validation Accuracy: 0.08474576473236084\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model2.fit(\n",
    "    train_generator,\n",
    "    epochs=30,  # Adjust the number of epochs as needed\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on validation set\n",
    "val_loss, val_acc = model2.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KyJrgcI-7ZJ5"
   },
   "outputs": [],
   "source": [
    "# Load ResNet50 pre-trained model, excluding the top layer (for classification)\n",
    "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "# Freeze the ResNet50 layers to avoid retraining them\n",
    "for layer in resnet50_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of ResNet50 base\n",
    "x = resnet50_base.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_tensor = Dense(30, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model3 = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "# Compile and train\n",
    "model3.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 142450,
     "status": "ok",
     "timestamp": 1728083702579,
     "user": {
      "displayName": "priyanshu singh",
      "userId": "03469869412728691574"
     },
     "user_tz": -330
    },
    "id": "XarFPrys7ZHi",
    "outputId": "e47a9682-c781-4b21-89d2-13d75a13838f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 1s/step - accuracy: 0.0364 - loss: 3.9565 - val_accuracy: 0.0282 - val_loss: 3.4120 - learning_rate: 1.0000e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 586ms/step - accuracy: 0.0289 - loss: 3.4085 - val_accuracy: 0.0339 - val_loss: 3.4036 - learning_rate: 1.0000e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 578ms/step - accuracy: 0.0476 - loss: 3.3998 - val_accuracy: 0.0452 - val_loss: 3.3998 - learning_rate: 1.0000e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 588ms/step - accuracy: 0.0483 - loss: 3.4011 - val_accuracy: 0.0452 - val_loss: 3.4008 - learning_rate: 1.0000e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 424ms/step - accuracy: 0.0533 - loss: 3.4018 - val_accuracy: 0.0452 - val_loss: 3.4006 - learning_rate: 1.0000e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m20/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 430ms/step - accuracy: 0.0444 - loss: 3.4010\n",
      "Epoch 6: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 479ms/step - accuracy: 0.0447 - loss: 3.4010 - val_accuracy: 0.0452 - val_loss: 3.4010 - learning_rate: 1.0000e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 510ms/step - accuracy: 0.0390 - loss: 3.4010 - val_accuracy: 0.0452 - val_loss: 3.4010 - learning_rate: 2.0000e-05\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 204ms/step - accuracy: 0.0323 - loss: 3.4078\n",
      "Validation Loss: 3.4119927883148193, Validation Accuracy: 0.028248587623238564\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model3.fit(\n",
    "    train_generator,\n",
    "    epochs=30,  # Adjust the number of epochs as needed\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model on validation set\n",
    "val_loss, val_acc = model3.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ot6bQNuq7ZE5"
   },
   "outputs": [],
   "source": [
    "# Load EfficientNetB0 pre-trained model, excluding the top layer (for classification)\n",
    "efficientnet_base = EfficientNetB0(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "# Freeze the EfficientNetB0 layers to avoid retraining them\n",
    "for layer in efficientnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of EfficientNetB0 base\n",
    "x = efficientnet_base.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_tensor = Dense(30, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model4 = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "# Compile and train\n",
    "model4.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 181470,
     "status": "ok",
     "timestamp": 1728083919168,
     "user": {
      "displayName": "priyanshu singh",
      "userId": "03469869412728691574"
     },
     "user_tz": -330
    },
    "id": "GU2Spq3h7ZCN",
    "outputId": "941f1585-38e0-4ada-d86c-7812e442c718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 2s/step - accuracy: 0.0416 - loss: 4.0546 - val_accuracy: 0.0395 - val_loss: 3.4017 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 485ms/step - accuracy: 0.0223 - loss: 3.4042 - val_accuracy: 0.0452 - val_loss: 3.4011 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 545ms/step - accuracy: 0.0307 - loss: 3.4073 - val_accuracy: 0.0452 - val_loss: 3.4011 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 544ms/step - accuracy: 0.0460 - loss: 3.4012 - val_accuracy: 0.0282 - val_loss: 3.4139 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m20/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 442ms/step - accuracy: 0.0329 - loss: 3.4096\n",
      "Epoch 5: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 486ms/step - accuracy: 0.0337 - loss: 3.4094 - val_accuracy: 0.0395 - val_loss: 3.4015 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 526ms/step - accuracy: 0.0489 - loss: 3.4037 - val_accuracy: 0.0452 - val_loss: 3.4010 - learning_rate: 2.0000e-05\n",
      "Epoch 7/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 573ms/step - accuracy: 0.0494 - loss: 3.4010 - val_accuracy: 0.0452 - val_loss: 3.4010 - learning_rate: 2.0000e-05\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - accuracy: 0.0291 - loss: 3.4009\n",
      "Validation Loss: 3.401690721511841, Validation Accuracy: 0.03954802080988884\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model4.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_acc = model4.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrThs0d-7Y_5"
   },
   "outputs": [],
   "source": [
    "# Load MobileNetV2 pre-trained model, excluding the top layer (for classification)\n",
    "mobilenetv2_base = MobileNetV2(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "# Freeze the MobileNetV2 layers to avoid retraining them\n",
    "for layer in mobilenetv2_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of MobileNetV2 base\n",
    "x = mobilenetv2_base.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_tensor = Dense(30, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model5 = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "# Compile and train\n",
    "model5.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 373916,
     "status": "ok",
     "timestamp": 1728084320095,
     "user": {
      "displayName": "priyanshu singh",
      "userId": "03469869412728691574"
     },
     "user_tz": -330
    },
    "id": "Hd_bYwVfY82K",
    "outputId": "54d23e4d-7d25-4605-b936-041712efcaac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 814ms/step - accuracy: 0.0864 - loss: 4.7396 - val_accuracy: 0.2203 - val_loss: 2.7934 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 459ms/step - accuracy: 0.1613 - loss: 3.0188 - val_accuracy: 0.3446 - val_loss: 2.3819 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 410ms/step - accuracy: 0.2104 - loss: 2.7291 - val_accuracy: 0.4407 - val_loss: 2.1064 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 433ms/step - accuracy: 0.3415 - loss: 2.3665 - val_accuracy: 0.4689 - val_loss: 1.8074 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 486ms/step - accuracy: 0.4070 - loss: 2.2103 - val_accuracy: 0.5706 - val_loss: 1.5777 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 481ms/step - accuracy: 0.4563 - loss: 1.9156 - val_accuracy: 0.6215 - val_loss: 1.5296 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 416ms/step - accuracy: 0.4817 - loss: 1.8670 - val_accuracy: 0.5932 - val_loss: 1.4985 - learning_rate: 1.0000e-04\n",
      "Epoch 8/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 421ms/step - accuracy: 0.5518 - loss: 1.6086 - val_accuracy: 0.6554 - val_loss: 1.3313 - learning_rate: 1.0000e-04\n",
      "Epoch 9/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 411ms/step - accuracy: 0.5926 - loss: 1.4010 - val_accuracy: 0.6328 - val_loss: 1.3318 - learning_rate: 1.0000e-04\n",
      "Epoch 10/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 425ms/step - accuracy: 0.5667 - loss: 1.4771 - val_accuracy: 0.7006 - val_loss: 1.0890 - learning_rate: 1.0000e-04\n",
      "Epoch 11/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 496ms/step - accuracy: 0.6000 - loss: 1.4121 - val_accuracy: 0.6328 - val_loss: 1.2478 - learning_rate: 1.0000e-04\n",
      "Epoch 12/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 424ms/step - accuracy: 0.5738 - loss: 1.4629 - val_accuracy: 0.6893 - val_loss: 1.0896 - learning_rate: 1.0000e-04\n",
      "Epoch 13/20\n",
      "\u001b[1m20/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 464ms/step - accuracy: 0.6490 - loss: 1.2510\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 512ms/step - accuracy: 0.6482 - loss: 1.2534 - val_accuracy: 0.6893 - val_loss: 1.2342 - learning_rate: 1.0000e-04\n",
      "Epoch 14/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 476ms/step - accuracy: 0.6685 - loss: 1.1645 - val_accuracy: 0.7006 - val_loss: 1.1386 - learning_rate: 2.0000e-05\n",
      "Epoch 15/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 483ms/step - accuracy: 0.6828 - loss: 1.1212 - val_accuracy: 0.7006 - val_loss: 1.1055 - learning_rate: 2.0000e-05\n",
      "Epoch 16/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 487ms/step - accuracy: 0.7246 - loss: 0.9786 - val_accuracy: 0.6949 - val_loss: 1.0682 - learning_rate: 2.0000e-05\n",
      "Epoch 17/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 475ms/step - accuracy: 0.7449 - loss: 0.8799 - val_accuracy: 0.6949 - val_loss: 1.0571 - learning_rate: 2.0000e-05\n",
      "Epoch 18/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 456ms/step - accuracy: 0.7230 - loss: 0.9035 - val_accuracy: 0.7119 - val_loss: 1.0468 - learning_rate: 2.0000e-05\n",
      "Epoch 19/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 505ms/step - accuracy: 0.7203 - loss: 0.9790 - val_accuracy: 0.7232 - val_loss: 0.9859 - learning_rate: 2.0000e-05\n",
      "Epoch 20/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 495ms/step - accuracy: 0.7215 - loss: 0.9105 - val_accuracy: 0.7119 - val_loss: 1.0326 - learning_rate: 2.0000e-05\n",
      "Restoring model weights from the end of the best epoch: 19.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 175ms/step - accuracy: 0.7448 - loss: 0.9381\n",
      "Validation Loss: 0.9858637452125549, Validation Accuracy: 0.7231638431549072\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model5.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_acc = model5.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWKtU59S7Y9K"
   },
   "outputs": [],
   "source": [
    "# Load NASNetMobile pre-trained model, excluding the top layer (for classification)\n",
    "nasnet_base = NASNetMobile(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "# Freeze the NASNetMobile layers to avoid retraining them\n",
    "for layer in nasnet_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of NASNetMobile base\n",
    "x = nasnet_base.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_tensor = Dense(30, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model6 = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "# Compile and train\n",
    "model6.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 192865,
     "status": "ok",
     "timestamp": 1728084518830,
     "user": {
      "displayName": "priyanshu singh",
      "userId": "03469869412728691574"
     },
     "user_tz": -330
    },
    "id": "tcuLfmx_Y39S",
    "outputId": "b6756a17-34e6-440d-b0c2-aa2f5cab120c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 2s/step - accuracy: 0.0626 - loss: 4.3999 - val_accuracy: 0.2373 - val_loss: 2.8500 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 513ms/step - accuracy: 0.1718 - loss: 2.9885 - val_accuracy: 0.3333 - val_loss: 2.5023 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 424ms/step - accuracy: 0.2675 - loss: 2.6486 - val_accuracy: 0.4294 - val_loss: 2.2107 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 490ms/step - accuracy: 0.3443 - loss: 2.3573 - val_accuracy: 0.4520 - val_loss: 2.0880 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 466ms/step - accuracy: 0.3921 - loss: 2.3015 - val_accuracy: 0.4520 - val_loss: 1.9472 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 518ms/step - accuracy: 0.4236 - loss: 2.1340 - val_accuracy: 0.4859 - val_loss: 1.8377 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 514ms/step - accuracy: 0.4342 - loss: 1.9772 - val_accuracy: 0.5141 - val_loss: 1.7797 - learning_rate: 1.0000e-04\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 194ms/step - accuracy: 0.2705 - loss: 2.8311\n",
      "Validation Loss: 2.8500137329101562, Validation Accuracy: 0.23728813230991364\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model6.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_acc = model6.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XCHQAsSx7Y6R"
   },
   "outputs": [],
   "source": [
    "# Load Xception pre-trained model, excluding the top layer (for classification)\n",
    "xception_base = Xception(weights='imagenet', include_top=False, input_tensor=input_tensor)\n",
    "\n",
    "# Freeze the Xception layers to avoid retraining them\n",
    "for layer in xception_base.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add custom layers on top of Xception base\n",
    "x = xception_base.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_tensor = Dense(30, activation='softmax')(x)\n",
    "\n",
    "# Create the model\n",
    "model7 = Model(inputs=input_tensor, outputs=output_tensor)\n",
    "\n",
    "# Compile and train\n",
    "model7.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model7.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 149289,
     "status": "ok",
     "timestamp": 1728084669670,
     "user": {
      "displayName": "priyanshu singh",
      "userId": "03469869412728691574"
     },
     "user_tz": -330
    },
    "id": "4N3bBs7o7Y3p",
    "outputId": "8fc265b2-729f-4726-fbac-b7f7b4860a65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 1s/step - accuracy: 0.1063 - loss: 3.6416 - val_accuracy: 0.4068 - val_loss: 2.3656 - learning_rate: 1.0000e-04\n",
      "Epoch 2/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 505ms/step - accuracy: 0.2530 - loss: 2.6715 - val_accuracy: 0.4633 - val_loss: 1.8794 - learning_rate: 1.0000e-04\n",
      "Epoch 3/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 500ms/step - accuracy: 0.3739 - loss: 2.2552 - val_accuracy: 0.5141 - val_loss: 1.7458 - learning_rate: 1.0000e-04\n",
      "Epoch 4/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 554ms/step - accuracy: 0.4722 - loss: 1.9466 - val_accuracy: 0.5706 - val_loss: 1.5841 - learning_rate: 1.0000e-04\n",
      "Epoch 5/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 521ms/step - accuracy: 0.4916 - loss: 1.8185 - val_accuracy: 0.5367 - val_loss: 1.5951 - learning_rate: 1.0000e-04\n",
      "Epoch 6/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 501ms/step - accuracy: 0.4631 - loss: 1.8003 - val_accuracy: 0.6158 - val_loss: 1.4932 - learning_rate: 1.0000e-04\n",
      "Epoch 7/20\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 509ms/step - accuracy: 0.5600 - loss: 1.6301 - val_accuracy: 0.6045 - val_loss: 1.3544 - learning_rate: 1.0000e-04\n",
      "Epoch 7: early stopping\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 199ms/step - accuracy: 0.4686 - loss: 2.2775\n",
      "Validation Loss: 2.3656301498413086, Validation Accuracy: 0.4067796468734741\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model7.fit(\n",
    "    train_generator,\n",
    "    epochs=20,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=[reduce_lr, early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "val_loss, val_acc = model7.evaluate(val_generator)\n",
    "print(f\"Validation Loss: {val_loss}, Validation Accuracy: {val_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2835,
     "status": "ok",
     "timestamp": 1728085699105,
     "user": {
      "displayName": "priyanshu singh",
      "userId": "03469869412728691574"
     },
     "user_tz": -330
    },
    "id": "m7Aah7Q07Y1h",
    "outputId": "620a7358-0aae-40c4-d198-2bf8af1c8891"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at /content/drive/MyDrive/Agriculture Classifier/Model/MobileNetV2.h5\n"
     ]
    }
   ],
   "source": [
    "# Define the directory path\n",
    "model_dir = '/content/drive/MyDrive/Agriculture Classifier/Model'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the model in the \"Model\" folder\n",
    "model5.save(os.path.join(model_dir, 'MobileNetV2.h5'))\n",
    "\n",
    "print(f\"Model saved at {os.path.join(model_dir, 'MobileNetV2.h5')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsylmnR27Ymx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eo9WkFL87YkC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2LMM5GkL7Yhr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uS2l7Lam7Ye5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bsOgK0Er7Ych"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_6r3eNvN7YaS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYSa01uq7YXl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wIIwWlyw7YVh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rff6sHVF7YSy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7E8cxqxZ7YQ5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R6_gVVPq7YOB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZsDNFNL7YLp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-0Xxs4h7YJR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t397AWqH7YGp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8dQv4ydy7YEI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjDy0Mku7YBb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kD7qF5IQ7X_J"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqVeb1pv7X8r"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JcpQxaiH7X5p"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2QJFUqN7X2x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57GtqvWU7X0S"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a_dRnxAk7Xxz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fzcmjn5A7Xvh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJHGH6SC7XtD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WCD5k2-U7XqR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjDl8b2Y7XoB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAo5Uxa57XlZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TxnCwuzh7Xi-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6VeKn2NC7XgZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHNlYUL_7Xdx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1v8TAe4H7XbS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfnFZ2QS7XYh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FwyGWDuP7XVq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNKhV85MUd2iXY7xMONo3Da",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Agri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
